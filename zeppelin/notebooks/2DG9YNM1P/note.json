{
  "paragraphs": [
    {
      "text": "%md\n# Anomaly Detection\n#### author: Loan Lassalle",
      "user": "anonymous",
      "dateUpdated": "Jul 7, 2018 4:21:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eAnomaly Detection\u003c/h1\u003e\n\u003ch4\u003eauthor: Loan Lassalle\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998491856_-2147449320",
      "id": "20180626-073451_426975331",
      "dateCreated": "Jun 26, 2018 7:34:51 AM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Loading data",
      "user": "anonymous",
      "dateUpdated": "Jul 7, 2018 4:22:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eLoading data\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530016014058_1331767191",
      "id": "20180626-122654_1915337111",
      "dateCreated": "Jun 26, 2018 12:26:54 PM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dataWithoutHeader \u003d spark\n  .read\n  .option(\"inferSchema\", true)\n  .option(\"header\", false)\n  .csv(\"/data/kddcup.data_10_percent_corrected\")\n\ndataWithoutHeader.count()",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "dataWithoutHeader: org.apache.spark.sql.DataFrame \u003d [_c0: int, _c1: string ... 40 more fields]\nres66: Long \u003d 494021\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998521426_-1879633437",
      "id": "20180626-073521_364480744",
      "dateCreated": "Jun 26, 2018 7:35:21 AM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val data \u003d dataWithoutHeader.toDF(\n  \"duration\", \"protocol_type\", \"service\", \"flag\",\n  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n  \"dst_host_count\", \"dst_host_srv_count\",\n  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n  \"label\")",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "data: org.apache.spark.sql.DataFrame \u003d [duration: int, protocol_type: string ... 40 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529999252462_-1147381304",
      "id": "20180626-074732_1808920546",
      "dateCreated": "Jun 26, 2018 7:47:32 AM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "data.select(\"label\")\n  .groupBy(\"label\")\n  .count()\n  .orderBy($\"count\".desc)\n  .show(25)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------------+------+\n|           label| count|\n+----------------+------+\n|          smurf.|280790|\n|        neptune.|107201|\n|         normal.| 97278|\n|           back.|  2203|\n|          satan.|  1589|\n|        ipsweep.|  1247|\n|      portsweep.|  1040|\n|    warezclient.|  1020|\n|       teardrop.|   979|\n|            pod.|   264|\n|           nmap.|   231|\n|   guess_passwd.|    53|\n|buffer_overflow.|    30|\n|           land.|    21|\n|    warezmaster.|    20|\n|           imap.|    12|\n|        rootkit.|    10|\n|     loadmodule.|     9|\n|      ftp_write.|     8|\n|       multihop.|     7|\n|            phf.|     4|\n|           perl.|     3|\n|            spy.|     2|\n+----------------+------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529999320415_711042372",
      "id": "20180626-074840_53027367",
      "dateCreated": "Jun 26, 2018 7:48:40 AM",
      "dateStarted": "Jun 26, 2018 8:23:20 PM",
      "dateFinished": "Jun 26, 2018 8:23:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### A First Take on Clustering",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eA First Take on Clustering\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998587544_1733466863",
      "id": "20180626-073627_2145790578",
      "dateCreated": "Jun 26, 2018 7:36:27 AM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.util.Random\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.DataFrame",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.util.Random\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530017890384_-514209891",
      "id": "20180626-125810_854618638",
      "dateCreated": "Jun 26, 2018 12:58:10 PM",
      "dateStarted": "Jun 26, 2018 8:23:20 PM",
      "dateFinished": "Jun 26, 2018 8:23:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val numericOnly \u003d data.drop(\"protocol_type\", \"service\", \"flag\").cache()",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [duration: int, src_bytes: int ... 37 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530015949846_858248564",
      "id": "20180626-122549_1014889477",
      "dateCreated": "Jun 26, 2018 12:25:49 PM",
      "dateStarted": "Jun 26, 2018 8:23:22 PM",
      "dateFinished": "Jun 26, 2018 8:23:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 0",
      "text": "def fitPipeline0(data: DataFrame): PipelineModel \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n\n  val kMeans \u003d new KMeans()\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringTake0(data: DataFrame): Unit \u003d {\n  val model \u003d fitPipeline0(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.clusterCenters.foreach(println)\n \n  val withCluster \u003d model.transform(numericOnly)\n  withCluster.select(\"cluster\", \"label\")\n    .groupBy(\"cluster\", \"label\").count()\n    .orderBy($\"cluster\", $\"count\".desc)\n    .show(25)\n}\n\nclusteringTake0(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline0: (data: org.apache.spark.sql.DataFrame)org.apache.spark.ml.PipelineModel\nclusteringTake0: (data: org.apache.spark.sql.DataFrame)Unit\n[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442943,0.17660780940042914,0.05743309987449898,0.05771839196793656,0.7915488441762945,0.020981640419421355,0.028996862475203923,232.4707319541719,188.6660459090725,0.7537812031901686,0.030905611108870867,0.6019355289259973,0.006683514837454898,0.17675395732966057,0.1764416217966883,0.05811762681672766,0.057411116958826745]\n[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n+-------+----------------+------+\n|cluster|           label| count|\n+-------+----------------+------+\n|      0|          smurf.|280790|\n|      0|        neptune.|107201|\n|      0|         normal.| 97278|\n|      0|           back.|  2203|\n|      0|          satan.|  1589|\n|      0|        ipsweep.|  1247|\n|      0|      portsweep.|  1039|\n|      0|    warezclient.|  1020|\n|      0|       teardrop.|   979|\n|      0|            pod.|   264|\n|      0|           nmap.|   231|\n|      0|   guess_passwd.|    53|\n|      0|buffer_overflow.|    30|\n|      0|           land.|    21|\n|      0|    warezmaster.|    20|\n|      0|           imap.|    12|\n|      0|        rootkit.|    10|\n|      0|     loadmodule.|     9|\n|      0|      ftp_write.|     8|\n|      0|       multihop.|     7|\n|      0|            phf.|     4|\n|      0|           perl.|     3|\n|      0|            spy.|     2|\n|      1|      portsweep.|     1|\n+-------+----------------+------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018658049_-89513859",
      "id": "20180626-131058_522806842",
      "dateCreated": "Jun 26, 2018 1:10:58 PM",
      "dateStarted": "Jun 26, 2018 8:23:22 PM",
      "dateFinished": "Jun 26, 2018 8:23:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Choosing k",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eChoosing k\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004121722_-591267022",
      "id": "20180626-090841_1946012157",
      "dateCreated": "Jun 26, 2018 9:08:41 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 1",
      "text": "def clusteringScore1(data: DataFrame, k: Int): Double \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  val model \u003d pipeline.fit(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(assembler.transform(data)) / data.count()\n}\n\ndef clusteringTake1(data: DataFrame): Unit \u003d {\n  (20 to 100 by 20)\n    .map(k \u003d\u003e (k, clusteringScore1(data, k)))\n    .foreach(println)\n}\n\nclusteringTake1(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "clusteringScore1: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake1: (data: org.apache.spark.sql.DataFrame)Unit\n(20,6.98891012074517E7)\n(40,5.5625820671694055E7)\n(60,1.6833250317251023E7)\n(80,3.414719346874615E7)\n(100,3.1472672371977188E7)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004360706_-1248297195",
      "id": "20180626-091240_948200329",
      "dateCreated": "Jun 26, 2018 9:12:40 AM",
      "dateStarted": "Jun 26, 2018 8:23:23 PM",
      "dateFinished": "Jun 26, 2018 8:24:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 2",
      "text": "def clusteringScore2(data: DataFrame, k: Int): Double \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  val model \u003d pipeline.fit(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(assembler.transform(data)) / data.count()\n}\n\ndef clusteringTake2(data: DataFrame): Unit \u003d {\n  (20 to 100 by 20)\n    .map(k \u003d\u003e (k, clusteringScore2(data, k)))\n    .foreach(println)\n}\n\nclusteringTake2(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "clusteringScore2: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake2: (data: org.apache.spark.sql.DataFrame)Unit\n(20,4.4226110562110074E7)\n(40,6840298.382733495)\n(60,3.889980443039568E7)\n(80,2.1001967744961016E7)\n(100,1.3229102748447115E7)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004481570_-6511635",
      "id": "20180626-091441_1574207042",
      "dateCreated": "Jun 26, 2018 9:14:41 AM",
      "dateStarted": "Jun 26, 2018 8:23:34 PM",
      "dateFinished": "Jun 26, 2018 8:26:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Feature Normalization",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFeature Normalization\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005396122_-646876132",
      "id": "20180626-092956_1818203621",
      "dateCreated": "Jun 26, 2018 9:29:56 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 3",
      "text": "def fitPipeline3(data: DataFrame, k: Int): PipelineModel \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringScore3(data: DataFrame, k: Int): Double \u003d {\n  val model \u003d fitPipeline3(data, k)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(model.transform(data)) / data.count()\n}\n\ndef clusteringTake3(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore3(numericOnly, k)))\n    .foreach(println)\n}\n\nclusteringTake3(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline3: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nclusteringScore3: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake3: (data: org.apache.spark.sql.DataFrame)Unit\n(60,1.0755318740294515)\n(90,0.7123900779437585)\n(120,0.445998034224042)\n(150,0.3849702986941831)\n(180,0.30270752612256)\n(210,0.252830478778997)\n(240,0.2253329890085924)\n(270,0.20380667376787137)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005692944_-1104285091",
      "id": "20180626-093452_110600592",
      "dateCreated": "Jun 26, 2018 9:34:52 AM",
      "dateStarted": "Jun 26, 2018 8:24:49 PM",
      "dateFinished": "Jun 26, 2018 8:32:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Categorical Variables",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCategorical Variables\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005951043_1517746371",
      "id": "20180626-093911_596564830",
      "dateCreated": "Jun 26, 2018 9:39:11 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 4",
      "text": "def oneHotPipeline(inputCol: String): (Pipeline, String) \u003d {\n  val indexer \u003d new StringIndexer()\n    .setInputCol(inputCol)\n    .setOutputCol(inputCol + \"_indexed\")\n  \n  val encoder \u003d new OneHotEncoder()\n    .setInputCol(inputCol + \"_indexed\")\n    .setOutputCol(inputCol + \"_vec\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(indexer, encoder))\n  (pipeline, inputCol + \"_vec\")\n}\n\ndef fitPipeline4(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n  \n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n      \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n    \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n    \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n    \n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringScore4(data: DataFrame, k: Int): Double \u003d {\n  val model \u003d fitPipeline4(data, k)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(model.transform(data)) / data.count()\n}\n\ndef clusteringTake4(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore4(data, k)))\n    .foreach(println)\n}\n\nclusteringTake4(data)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\nfitPipeline4: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nclusteringScore4: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake4: (data: org.apache.spark.sql.DataFrame)Unit\n(60,34.08603130933672)\n(90,11.586273580391165)\n(120,3.0439420182309074)\n(150,2.075924047308254)\n(180,1.4990140664036147)\n(210,1.1245749386192638)\n(240,0.9935802045806598)\n(270,0.8032256114599061)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530006686921_2065088048",
      "id": "20180626-095126_2012873590",
      "dateCreated": "Jun 26, 2018 9:51:26 AM",
      "dateStarted": "Jun 26, 2018 8:26:39 PM",
      "dateFinished": "Jun 26, 2018 8:48:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Using Labels with Entropy",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eUsing Labels with Entropy\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530006790029_-1006808467",
      "id": "20180626-095310_1526218671",
      "dateCreated": "Jun 26, 2018 9:53:10 AM",
      "dateStarted": "Jun 26, 2018 8:23:17 PM",
      "dateFinished": "Jun 26, 2018 8:23:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 5",
      "text": "def fitPipeline5(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n  \n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n      \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n    \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n    \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n    \n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef entropy(counts: Iterable[Int]): Double \u003d { \n  val values \u003d counts.filter(_ \u003e 0)\n  val n \u003d values.map(_.toDouble).sum\n    \n  values.map { v \u003d\u003e\n    val p \u003d v / n\n    -p * math.log(p)\n  }.sum\n}\n\ndef clusteringScore5(data: DataFrame, k: Int): Double \u003d {\n  val clusterLabel \u003d fitPipeline5(data, k).transform(data).select(\"cluster\", \"label\").as[(Int, String)]\n  \n  // Extract collections of labels, per cluster\n  val weightedClusterEntropy \u003d clusterLabel\n    .groupByKey { case (cluster, _) \u003d\u003e cluster }\n    .mapGroups { case (_, clusterLabels) \u003d\u003e\n      val labels \u003d clusterLabels.map { case (_, label) \u003d\u003e label }.toSeq \n      \n      // Count labels in collections\n      val labelCounts \u003d labels.groupBy(identity).values.map(_.size)\n      \n      labels.size * entropy(labelCounts)\n    }.collect()\n\n  // Average entropy weighted by cluster size\n  weightedClusterEntropy.sum / data.count()\n}\n\ndef clusteringTake5(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore5(data, k)))\n    .foreach(println)\n}\n\nclusteringTake5(data)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline5: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nentropy: (counts: Iterable[Int])Double\nclusteringScore5: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake5: (data: org.apache.spark.sql.DataFrame)Unit\n(60,0.08391199125828215)\n(90,0.04615017531052519)\n(120,0.04112054627766675)\n(150,0.0241270759559924)\n(180,0.030638826065811007)\n(210,0.01273066383650454)\n(240,0.00809015624648769)\n(270,0.010265355505771314)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530007475540_979245098",
      "id": "20180626-100435_1181734863",
      "dateCreated": "Jun 26, 2018 10:04:35 AM",
      "dateStarted": "Jun 26, 2018 8:32:40 PM",
      "dateFinished": "Jun 26, 2018 9:05:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Intrusion Detection",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eIntrusion Detection\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018206824_-552242548",
      "id": "20180626-130326_301574963",
      "dateCreated": "Jun 26, 2018 1:03:26 PM",
      "dateStarted": "Jun 26, 2018 8:23:17 PM",
      "dateFinished": "Jun 26, 2018 8:23:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "def fitPipeline(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n\n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n  \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n\n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n\n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n  \ndef intrusionDetector(trainingData: DataFrame, testData: DataFrame): Unit \u003d {\n  val model \u003d fitPipeline(trainingData, 210)\n  model.save(\"/data/spark-k-means-model\")\n\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  val centroids \u003d kMeansModel.clusterCenters\n\n  val clustered \u003d model.transform(testData)\n  val threshold \u003d clustered.select(\"cluster\", \"scaledFeatureVector\").as[(Int, Vector)]\n    .map { case (cluster, vec) \u003d\u003e Vectors.sqdist(centroids(cluster), vec) }\n    .orderBy($\"value\".desc)\n    .take(100)\n    .last\n\n  val originalCols \u003d trainingData.columns\n  val anomalies \u003d clustered.filter { row \u003d\u003e\n    val cluster \u003d row.getAs[Int](\"cluster\")\n    val vec \u003d row.getAs[Vector](\"scaledFeatureVector\")\n    Vectors.sqdist(centroids(cluster), vec) \u003e\u003d threshold\n  }.select(originalCols.head, originalCols.tail:_*)\n\n  println(anomalies.first())\n}\n\nval trainingData \u003d spark\n  .read\n  .option(\"inferSchema\", true)\n  .option(\"header\", false)\n  .csv(\"/data/kddcup.data_10_percent_corrected\")\n  .toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n    \"dst_host_count\", \"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n    \"label\")\n\nval testData \u003d spark\n  .read\n  .option(\"inferSchema\", true)\n  .option(\"header\", false)\n  .csv(\"/data/kddcup.testdata.unlabeled_10_percent\")\n  .toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n    \"dst_host_count\", \"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\")\n  \nintrusionDetector(trainingData, testData)",
      "user": "anonymous",
      "dateUpdated": "Jun 27, 2018 7:55:07 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nintrusionDetector: (trainingData: org.apache.spark.sql.DataFrame, testData: org.apache.spark.sql.DataFrame)Unit\ntrainingData: org.apache.spark.sql.DataFrame \u003d [duration: int, protocol_type: string ... 40 more fields]\ntestData: org.apache.spark.sql.DataFrame \u003d [duration: int, protocol_type: string ... 39 more fields]\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5983.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5983.0 (TID 16974, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) \u003d\u003e double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n\tat org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:670)\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1419)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: icmp.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n\t... 20 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n  at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1428)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1415)\n  at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:133)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n  at intrusionDetector(\u003cconsole\u003e:84)\n  ... 46 elided\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$4: (string) \u003d\u003e double)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)\n  at org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:670)\n  at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)\n  at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1422)\n  at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1419)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n  ... 3 more\nCaused by: org.apache.spark.SparkException: Unseen label: icmp.\n  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:170)\n  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:166)\n  ... 20 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018281131_-86773000",
      "id": "20180626-130441_1307172991",
      "dateCreated": "Jun 26, 2018 1:04:41 PM",
      "dateStarted": "Jun 26, 2018 9:18:21 PM",
      "dateFinished": "Jun 26, 2018 9:20:49 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1530044597224_-1771895964",
      "id": "20180626-202317_2005299692",
      "dateCreated": "Jun 26, 2018 8:23:17 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Anomaly Detection",
  "id": "2DG9YNM1P",
  "angularObjects": {
    "2DM27M45M:shared_process": [],
    "2DJH3P7XC:shared_process": [],
    "2DHDUN7WB:shared_process": [],
    "2DHAUUWKR:shared_process": [],
    "2DHJFQVB5:shared_process": [],
    "2DHDP35M3:shared_process": [],
    "2DKQDT9RU:shared_process": [],
    "2DKA1YMFH:shared_process": [],
    "2DJ995EAE:shared_process": [],
    "2DMBB58XT:shared_process": [],
    "2DHAG8E8Q:shared_process": [],
    "2DM9SXH1F:shared_process": [],
    "2DHZEB4BR:shared_process": [],
    "2DKQ1NQ71:shared_process": [],
    "2DJQ5BS5E:shared_process": [],
    "2DGUW1UU5:shared_process": [],
    "2DJR434H7:shared_process": [],
    "2DM2BHWFQ:shared_process": [],
    "2DKWCBGZF:shared_process": []
  },
  "config": {},
  "info": {}
}