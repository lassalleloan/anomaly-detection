{
  "paragraphs": [
    {
      "text": "%md\n# Anomaly Detection\n#### author: Loan Lassalle",
      "user": "anonymous",
      "dateUpdated": "Sep 10, 2018 12:08:34 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eAnomaly Detection\u003c/h1\u003e\n\u003ch4\u003eauthor: Loan Lassalle\u003c/h4\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998491856_-2147449320",
      "id": "20180626-073451_426975331",
      "dateCreated": "Jun 26, 2018 7:34:51 AM",
      "dateStarted": "Sep 10, 2018 12:08:35 PM",
      "dateFinished": "Sep 10, 2018 12:08:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Loading data",
      "user": "anonymous",
      "dateUpdated": "Sep 10, 2018 12:08:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eLoading data\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530016014058_1331767191",
      "id": "20180626-122654_1915337111",
      "dateCreated": "Jun 26, 2018 12:26:54 PM",
      "dateStarted": "Sep 10, 2018 12:08:35 PM",
      "dateFinished": "Sep 10, 2018 12:08:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val dataWithoutHeader \u003d spark\n  .read\n  .option(\"inferSchema\", true)\n  .option(\"header\", false)\n  .csv(\"/data/kddcup_1999_network_dataset/kddcup.data_10_percent_corrected\")\n\ndataWithoutHeader.count()",
      "user": "anonymous",
      "dateUpdated": "Sep 8, 2018 5:41:13 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "tableHide": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "dataWithoutHeader: org.apache.spark.sql.DataFrame \u003d [_c0: int, _c1: string ... 40 more fields]\nres1: Long \u003d 494021\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998521426_-1879633437",
      "id": "20180626-073521_364480744",
      "dateCreated": "Jun 26, 2018 7:35:21 AM",
      "dateStarted": "Sep 8, 2018 5:41:14 PM",
      "dateFinished": "Sep 8, 2018 5:41:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val data \u003d dataWithoutHeader.toDF(\n  \"duration\", \"protocol_type\", \"service\", \"flag\",\n  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n  \"dst_host_count\", \"dst_host_srv_count\",\n  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n  \"label\")",
      "user": "anonymous",
      "dateUpdated": "Jul 25, 2018 12:05:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "data: org.apache.spark.sql.DataFrame \u003d [duration: int, protocol_type: string ... 40 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529999252462_-1147381304",
      "id": "20180626-074732_1808920546",
      "dateCreated": "Jun 26, 2018 7:47:32 AM",
      "dateStarted": "Jul 25, 2018 12:05:50 AM",
      "dateFinished": "Jul 25, 2018 12:05:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "data.select(\"label\")\n  .groupBy(\"label\")\n  .count()\n  .orderBy($\"count\".desc)\n  .show(25)",
      "user": "anonymous",
      "dateUpdated": "Jul 25, 2018 12:06:03 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----------------+------+\n|           label| count|\n+----------------+------+\n|          smurf.|280790|\n|        neptune.|107201|\n|         normal.| 97278|\n|           back.|  2203|\n|          satan.|  1589|\n|        ipsweep.|  1247|\n|      portsweep.|  1040|\n|    warezclient.|  1020|\n|       teardrop.|   979|\n|            pod.|   264|\n|           nmap.|   231|\n|   guess_passwd.|    53|\n|buffer_overflow.|    30|\n|           land.|    21|\n|    warezmaster.|    20|\n|           imap.|    12|\n|        rootkit.|    10|\n|     loadmodule.|     9|\n|      ftp_write.|     8|\n|       multihop.|     7|\n|            phf.|     4|\n|           perl.|     3|\n|            spy.|     2|\n+----------------+------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529999320415_711042372",
      "id": "20180626-074840_53027367",
      "dateCreated": "Jun 26, 2018 7:48:40 AM",
      "dateStarted": "Jul 25, 2018 12:06:03 AM",
      "dateFinished": "Jul 25, 2018 12:06:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### A First Take on Clustering",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eA First Take on Clustering\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1529998587544_1733466863",
      "id": "20180626-073627_2145790578",
      "dateCreated": "Jun 26, 2018 7:36:27 AM",
      "dateStarted": "Jun 26, 2018 8:23:15 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import scala.util.Random\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.DataFrame",
      "user": "anonymous",
      "dateUpdated": "Sep 10, 2018 2:18:25 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.util.Random\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530017890384_-514209891",
      "id": "20180626-125810_854618638",
      "dateCreated": "Jun 26, 2018 12:58:10 PM",
      "dateStarted": "Jul 25, 2018 12:05:56 AM",
      "dateFinished": "Jul 25, 2018 12:05:57 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val numericOnly \u003d data.drop(\"protocol_type\", \"service\", \"flag\").cache()",
      "user": "anonymous",
      "dateUpdated": "Jul 25, 2018 12:05:59 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [duration: int, src_bytes: int ... 37 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530015949846_858248564",
      "id": "20180626-122549_1014889477",
      "dateCreated": "Jun 26, 2018 12:25:49 PM",
      "dateStarted": "Jul 25, 2018 12:05:59 AM",
      "dateFinished": "Jul 25, 2018 12:05:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 0",
      "text": "def fitPipeline0(data: DataFrame): PipelineModel \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n\n  val kMeans \u003d new KMeans()\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringTake0(data: DataFrame): Unit \u003d {\n  val model \u003d fitPipeline0(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.clusterCenters.foreach(println)\n \n  val withCluster \u003d model.transform(numericOnly)\n  withCluster.select(\"cluster\", \"label\")\n    .groupBy(\"cluster\", \"label\").count()\n    .orderBy($\"cluster\", $\"count\".desc)\n    .show(25)\n}\n\nclusteringTake0(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline0: (data: org.apache.spark.sql.DataFrame)org.apache.spark.ml.PipelineModel\nclusteringTake0: (data: org.apache.spark.sql.DataFrame)Unit\n[47.979395571029514,1622.078830816566,868.5341828266062,4.453261001578883E-5,0.006432937937735314,1.4169466823205539E-5,0.03451682118132869,1.5181571596291647E-4,0.14824703453301485,0.01021213716043885,1.1133152503947209E-4,3.6435771831099954E-5,0.011351767134933808,0.0010829521072021374,1.0930731549329986E-4,0.0010080563539937655,0.0,0.0,0.0013865835391279706,332.2862475203433,292.9071434354884,0.17668541759442943,0.17660780940042914,0.05743309987449898,0.05771839196793656,0.7915488441762945,0.020981640419421355,0.028996862475203923,232.4707319541719,188.6660459090725,0.7537812031901686,0.030905611108870867,0.6019355289259973,0.006683514837454898,0.17675395732966057,0.1764416217966883,0.05811762681672766,0.057411116958826745]\n[2.0,6.9337564E8,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,57.0,3.0,0.79,0.67,0.21,0.33,0.05,0.39,0.0,255.0,3.0,0.01,0.09,0.22,0.0,0.18,0.67,0.05,0.33]\n+-------+----------------+------+\n|cluster|           label| count|\n+-------+----------------+------+\n|      0|          smurf.|280790|\n|      0|        neptune.|107201|\n|      0|         normal.| 97278|\n|      0|           back.|  2203|\n|      0|          satan.|  1589|\n|      0|        ipsweep.|  1247|\n|      0|      portsweep.|  1039|\n|      0|    warezclient.|  1020|\n|      0|       teardrop.|   979|\n|      0|            pod.|   264|\n|      0|           nmap.|   231|\n|      0|   guess_passwd.|    53|\n|      0|buffer_overflow.|    30|\n|      0|           land.|    21|\n|      0|    warezmaster.|    20|\n|      0|           imap.|    12|\n|      0|        rootkit.|    10|\n|      0|     loadmodule.|     9|\n|      0|      ftp_write.|     8|\n|      0|       multihop.|     7|\n|      0|            phf.|     4|\n|      0|           perl.|     3|\n|      0|            spy.|     2|\n|      1|      portsweep.|     1|\n+-------+----------------+------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018658049_-89513859",
      "id": "20180626-131058_522806842",
      "dateCreated": "Jun 26, 2018 1:10:58 PM",
      "dateStarted": "Jun 26, 2018 8:23:22 PM",
      "dateFinished": "Jun 26, 2018 8:23:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Choosing k",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eChoosing k\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004121722_-591267022",
      "id": "20180626-090841_1946012157",
      "dateCreated": "Jun 26, 2018 9:08:41 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 1",
      "text": "def clusteringScore1(data: DataFrame, k: Int): Double \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  val model \u003d pipeline.fit(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(assembler.transform(data)) / data.count()\n}\n\ndef clusteringTake1(data: DataFrame): Unit \u003d {\n  (20 to 100 by 20)\n    .map(k \u003d\u003e (k, clusteringScore1(data, k)))\n    .foreach(println)\n}\n\nclusteringTake1(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "clusteringScore1: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake1: (data: org.apache.spark.sql.DataFrame)Unit\n(20,6.98891012074517E7)\n(40,5.5625820671694055E7)\n(60,1.6833250317251023E7)\n(80,3.414719346874615E7)\n(100,3.1472672371977188E7)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004360706_-1248297195",
      "id": "20180626-091240_948200329",
      "dateCreated": "Jun 26, 2018 9:12:40 AM",
      "dateStarted": "Jun 26, 2018 8:23:23 PM",
      "dateFinished": "Jun 26, 2018 8:24:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 2",
      "text": "def clusteringScore2(data: DataFrame, k: Int): Double \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"featureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, kMeans))\n  val model \u003d pipeline.fit(data)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(assembler.transform(data)) / data.count()\n}\n\ndef clusteringTake2(data: DataFrame): Unit \u003d {\n  (20 to 100 by 20)\n    .map(k \u003d\u003e (k, clusteringScore2(data, k)))\n    .foreach(println)\n}\n\nclusteringTake2(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "clusteringScore2: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake2: (data: org.apache.spark.sql.DataFrame)Unit\n(20,4.4226110562110074E7)\n(40,6840298.382733495)\n(60,3.889980443039568E7)\n(80,2.1001967744961016E7)\n(100,1.3229102748447115E7)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530004481570_-6511635",
      "id": "20180626-091441_1574207042",
      "dateCreated": "Jun 26, 2018 9:14:41 AM",
      "dateStarted": "Jun 26, 2018 8:23:34 PM",
      "dateFinished": "Jun 26, 2018 8:26:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Feature Normalization",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFeature Normalization\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005396122_-646876132",
      "id": "20180626-092956_1818203621",
      "dateCreated": "Jun 26, 2018 9:29:56 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 3",
      "text": "def fitPipeline3(data: DataFrame, k: Int): PipelineModel \u003d {\n  val assembler \u003d new VectorAssembler()\n    .setInputCols(data.columns.filter(_ !\u003d \"label\"))\n    .setOutputCol(\"featureVector\")\n        \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n        \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n        \n  val pipeline \u003d new Pipeline().setStages(Array(assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringScore3(data: DataFrame, k: Int): Double \u003d {\n  val model \u003d fitPipeline3(data, k)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(model.transform(data)) / data.count()\n}\n\ndef clusteringTake3(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore3(numericOnly, k)))\n    .foreach(println)\n}\n\nclusteringTake3(numericOnly)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline3: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nclusteringScore3: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake3: (data: org.apache.spark.sql.DataFrame)Unit\n(60,1.0755318740294515)\n(90,0.7123900779437585)\n(120,0.445998034224042)\n(150,0.3849702986941831)\n(180,0.30270752612256)\n(210,0.252830478778997)\n(240,0.2253329890085924)\n(270,0.20380667376787137)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005692944_-1104285091",
      "id": "20180626-093452_110600592",
      "dateCreated": "Jun 26, 2018 9:34:52 AM",
      "dateStarted": "Jun 26, 2018 8:24:49 PM",
      "dateFinished": "Jun 26, 2018 8:32:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Categorical Variables",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCategorical Variables\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530005951043_1517746371",
      "id": "20180626-093911_596564830",
      "dateCreated": "Jun 26, 2018 9:39:11 AM",
      "dateStarted": "Jun 26, 2018 8:23:16 PM",
      "dateFinished": "Jun 26, 2018 8:23:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 4",
      "text": "def oneHotPipeline(inputCol: String): (Pipeline, String) \u003d {\n  val indexer \u003d new StringIndexer()\n    .setInputCol(inputCol)\n    .setOutputCol(inputCol + \"_indexed\")\n  \n  val encoder \u003d new OneHotEncoder()\n    .setInputCol(inputCol + \"_indexed\")\n    .setOutputCol(inputCol + \"_vec\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(indexer, encoder))\n  (pipeline, inputCol + \"_vec\")\n}\n\ndef fitPipeline4(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n  \n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n      \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n    \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n    \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n    \n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef clusteringScore4(data: DataFrame, k: Int): Double \u003d {\n  val model \u003d fitPipeline4(data, k)\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  kMeansModel.computeCost(model.transform(data)) / data.count()\n}\n\ndef clusteringTake4(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore4(data, k)))\n    .foreach(println)\n}\n\nclusteringTake4(data)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\nfitPipeline4: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nclusteringScore4: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake4: (data: org.apache.spark.sql.DataFrame)Unit\n(60,34.08603130933672)\n(90,11.586273580391165)\n(120,3.0439420182309074)\n(150,2.075924047308254)\n(180,1.4990140664036147)\n(210,1.1245749386192638)\n(240,0.9935802045806598)\n(270,0.8032256114599061)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530006686921_2065088048",
      "id": "20180626-095126_2012873590",
      "dateCreated": "Jun 26, 2018 9:51:26 AM",
      "dateStarted": "Jun 26, 2018 8:26:39 PM",
      "dateFinished": "Jun 26, 2018 8:48:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Using Labels with Entropy",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eUsing Labels with Entropy\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530006790029_-1006808467",
      "id": "20180626-095310_1526218671",
      "dateCreated": "Jun 26, 2018 9:53:10 AM",
      "dateStarted": "Jun 26, 2018 8:23:17 PM",
      "dateFinished": "Jun 26, 2018 8:23:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Clustering, Take 5",
      "text": "def fitPipeline5(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n  \n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n      \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n    \n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n    \n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n    \n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n\ndef entropy(counts: Iterable[Int]): Double \u003d { \n  val values \u003d counts.filter(_ \u003e 0)\n  val n \u003d values.map(_.toDouble).sum\n    \n  values.map { v \u003d\u003e\n    val p \u003d v / n\n    -p * math.log(p)\n  }.sum\n}\n\ndef clusteringScore5(data: DataFrame, k: Int): Double \u003d {\n  val clusterLabel \u003d fitPipeline5(data, k).transform(data).select(\"cluster\", \"label\").as[(Int, String)]\n  \n  // Extract collections of labels, per cluster\n  val weightedClusterEntropy \u003d clusterLabel\n    .groupByKey { case (cluster, _) \u003d\u003e cluster }\n    .mapGroups { case (_, clusterLabels) \u003d\u003e\n      val labels \u003d clusterLabels.map { case (_, label) \u003d\u003e label }.toSeq \n      \n      // Count labels in collections\n      val labelCounts \u003d labels.groupBy(identity).values.map(_.size)\n      \n      labels.size * entropy(labelCounts)\n    }.collect()\n\n  // Average entropy weighted by cluster size\n  weightedClusterEntropy.sum / data.count()\n}\n\ndef clusteringTake5(data: DataFrame): Unit \u003d {\n  (60 to 270 by 30)\n    .map(k \u003d\u003e (k, clusteringScore5(data, k)))\n    .foreach(println)\n}\n\nclusteringTake5(data)",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "fitPipeline5: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nentropy: (counts: Iterable[Int])Double\nclusteringScore5: (data: org.apache.spark.sql.DataFrame, k: Int)Double\nclusteringTake5: (data: org.apache.spark.sql.DataFrame)Unit\n(60,0.08391199125828215)\n(90,0.04615017531052519)\n(120,0.04112054627766675)\n(150,0.0241270759559924)\n(180,0.030638826065811007)\n(210,0.01273066383650454)\n(240,0.00809015624648769)\n(270,0.010265355505771314)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530007475540_979245098",
      "id": "20180626-100435_1181734863",
      "dateCreated": "Jun 26, 2018 10:04:35 AM",
      "dateStarted": "Jun 26, 2018 8:32:40 PM",
      "dateFinished": "Jun 26, 2018 9:05:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Intrusion Detection",
      "user": "anonymous",
      "dateUpdated": "Jun 26, 2018 8:23:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eIntrusion Detection\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018206824_-552242548",
      "id": "20180626-130326_301574963",
      "dateCreated": "Jun 26, 2018 1:03:26 PM",
      "dateStarted": "Jun 26, 2018 8:23:17 PM",
      "dateFinished": "Jun 26, 2018 8:23:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "def oneHotPipeline(inputCol: String): (Pipeline, String) \u003d {\n  val indexer \u003d new StringIndexer()\n    .setInputCol(inputCol)\n    .setOutputCol(inputCol + \"_indexed\")\n  \n  val encoder \u003d new OneHotEncoder()\n    .setInputCol(inputCol + \"_indexed\")\n    .setOutputCol(inputCol + \"_vec\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(indexer, encoder))\n  (pipeline, inputCol + \"_vec\")\n}\n\ndef fitPipeline(data: DataFrame, k: Int): PipelineModel \u003d {\n  val (protoTypeEncoder, protoTypeVecCol) \u003d oneHotPipeline(\"protocol_type\")\n  val (serviceEncoder, serviceVecCol) \u003d oneHotPipeline(\"service\")\n  val (flagEncoder, flagVecCol) \u003d oneHotPipeline(\"flag\")\n\n  // Original columns, without label / string columns, but with new vector encoded cols\n  val assembleCols \u003d Set(data.columns: _*) -- \n    Seq(\"label\", \"protocol_type\", \"service\", \"flag\") ++\n    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)\n  \n  val assembler \u003d new VectorAssembler()\n    .setInputCols(assembleCols.toArray)\n    .setOutputCol(\"featureVector\")\n\n  val scaler \u003d new StandardScaler()\n    .setWithStd(true)\n    .setWithMean(false)\n    .setInputCol(\"featureVector\")\n    .setOutputCol(\"scaledFeatureVector\")\n\n  val kMeans \u003d new KMeans()\n    .setSeed(Random.nextLong())\n    .setK(k)\n    .setMaxIter(40)\n    .setTol(1.0e-5)\n    .setPredictionCol(\"cluster\")\n    .setFeaturesCol(\"scaledFeatureVector\")\n\n  val pipeline \u003d new Pipeline().setStages(Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kMeans))\n  pipeline.fit(data)\n}\n  \ndef intrusionDetector(dataframe: DataFrame): Unit \u003d {\n  val model \u003d fitPipeline(dataframe, 210)\n\n  val kMeansModel \u003d model.stages.last.asInstanceOf[KMeansModel]\n  val centroids \u003d kMeansModel.clusterCenters\n\n  val clustered \u003d model.transform(dataframe)\n  val threshold \u003d clustered.select(\"cluster\", \"scaledFeatureVector\").as[(Int, Vector)]\n    .map { case (cluster, vec) \u003d\u003e Vectors.sqdist(centroids(cluster), vec) }\n    .orderBy($\"value\".desc)\n    .take(100)\n    .last\n\n  val originalCols \u003d dataframe.columns\n  val anomalies \u003d clustered.filter { row \u003d\u003e\n    val cluster \u003d row.getAs[Int](\"cluster\")\n    val vec \u003d row.getAs[Vector](\"scaledFeatureVector\")\n    Vectors.sqdist(centroids(cluster), vec) \u003e\u003d threshold\n  }.select(originalCols.head, originalCols.tail:_*)\n\n  println(anomalies.first())\n}\n\nval dataframe \u003d spark\n  .read\n  .option(\"inferSchema\", true)\n  .option(\"header\", false)\n  .csv(\"/data/kddcup_1999_network_dataset/kddcup.data_10_percent_corrected\")\n  .toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n    \"dst_host_count\", \"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n    \"label\")\n  \nintrusionDetector(dataframe)",
      "user": "anonymous",
      "dateUpdated": "Jul 25, 2018 12:07:50 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/text",
        "editorHide": false,
        "tableHide": false,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\nfitPipeline: (data: org.apache.spark.sql.DataFrame, k: Int)org.apache.spark.ml.PipelineModel\nintrusionDetector: (dataframe: org.apache.spark.sql.DataFrame)Unit\ndataframe: org.apache.spark.sql.DataFrame \u003d [duration: int, protocol_type: string ... 40 more fields]\norg.apache.spark.SparkException: Job 23 cancelled part of cancelled job group zeppelin-20180626-130441_1307172991\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:748)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:747)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:747)\n  at org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:295)\n  at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:227)\n  at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:319)\n  at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:253)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n  at fitPipeline(\u003cconsole\u003e:67)\n  at intrusionDetector(\u003cconsole\u003e:41)\n  ... 46 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1530018281131_-86773000",
      "id": "20180626-130441_1307172991",
      "dateCreated": "Jun 26, 2018 1:04:41 PM",
      "dateStarted": "Jul 25, 2018 12:07:50 AM",
      "dateFinished": "Jul 25, 2018 12:08:37 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1530044597224_-1771895964",
      "id": "20180626-202317_2005299692",
      "dateCreated": "Jun 26, 2018 8:23:17 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Anomaly Detection Network",
  "id": "2DG9YNM1P",
  "angularObjects": {
    "2DQ2R5SBW:shared_process": [],
    "2DP2YMK8R:shared_process": [],
    "2DRZ16278:shared_process": [],
    "2DRMBQ2N2:shared_process": [],
    "2DQPGS39D:shared_process": [],
    "2DSNCPP5T:shared_process": [],
    "2DRY6GP18:shared_process": [],
    "2DQJXCFG7:shared_process": [],
    "2DQWVW3VU:shared_process": [],
    "2DPBFGYSY:shared_process": [],
    "2DRXNSKG1:shared_process": [],
    "2DRYRYVXP:shared_process": [],
    "2DSD3F65W:shared_process": [],
    "2DSYMFJ31:shared_process": [],
    "2DPGTDF6B:shared_process": [],
    "2DPR1K6QX:shared_process": [],
    "2DRAZZYJK:shared_process": [],
    "2DPWG5NPT:shared_process": [],
    "2DQQAAMM8:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}